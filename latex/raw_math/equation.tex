\documentclass[a4paper,10pt]{article}

\RequirePackage{color,graphicx}
\usepackage{amsmath,bm}
%--------------------BEGIN DOCUMENT----------------------
\begin{document}

\begin{bf}
\begin{center}
Transductive Adversarial Networks\\
Sean Rowan and Lewis Moffat
\end{center}
\end{bf}

\begin{itemize}
\item $p_s(x,z)$: The joint data distribution from the source domain
\item $p_s(z)$: The marginalised latent data distribution from the source domain
\item $p_t(x)$: The objective data distribution from the target domain
\item $\hat{p}_t(z)$: The latent prior distribution from the target domain
\item $G(x|z)$: The generator function
\item $E(z|x)$: The encoder function
\item $D_s(x,z)$: The source domain discriminator function
\item $D_t(x,z)$: The target domain discriminator function
\end{itemize}

The TAN framework is decomposed into a GAN framework (for the source domain) and an ALI framework (for the target domain), with a shared generator $G$ between the networks, to achieve the following optimality results:

\begin{equation}
p_s(x,z) = G(x|z)p_s(z)
\end{equation}

\begin{equation}
G(x|z)\hat{p}_t(z) = E(z|x)p_t(x)
\end{equation}
Therefore
\begin{equation}
\begin{split}
G(x|z) &= \frac{E(z|x)p_t(x)}{\hat{p}_t(z)}\\
&= \frac{p_s(x,z)}{p_s(z)}
\end{split}
\end{equation}
Finally
\begin{equation}
E(z|x) = \frac{p_s(x,z)\hat{p}_t(z)}{p_s(z)p_t(x)}
\end{equation}


There are two theoretically valid training procedures in the TAN framework:
\begin{enumerate}
\item The generator $G$ is only trained on the source domain data, and is trained to convergence before performing training of the encoder $E$ in the ALI framework (where $G$ is kept fixed).
\item The generator $G$ is trained on both the target and source domain data in alternating minibatch stochastic gradient descents of the GAN framework and the ALI framework. The TAN theoretical optimum still holds true in this case because the adversarial optimality requirement is only that the gradient descent can reach an optimum \textbf{with each individual minibatch}. Therefore each separate minibatch can be trained differently (i.e. alternating between the GAN and ALI frameworks) without effecting the theoretical optimality requirements of $G$. \emph{I suspect this will perform better experimentally due to a global optimum of the network parameters not likely being reached when using multilayer perceptrons; therefore this forces $G$ to accommodate both domains of data.}
\end{enumerate}

In experiments then, the training procedures can be regularised with additional reward terms added to the adversarial reward, stemming from prior knowledge about a particular problem.

\end{document}
